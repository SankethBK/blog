<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Sanketh's Blog</title><link>https://sankethbk.github.io/blog/</link><description>Recent content on Sanketh's Blog</description><generator>Hugo -- 0.148.2</generator><language>en-us</language><lastBuildDate>Fri, 18 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://sankethbk.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Processor Modes in x86</title><link>https://sankethbk.github.io/blog/posts/cpu/2025-07-20-processor-modes/</link><pubDate>Fri, 18 Jul 2025 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/cpu/2025-07-20-processor-modes/</guid><description>&lt;h1 id="the-8086-processor">The 8086 Processor&lt;/h1>
&lt;h2 id="a-brief-history">A Brief History&lt;/h2>
&lt;p>The Intel 8086, released in 1978, marked a pivotal moment in computing history as Intel&amp;rsquo;s first 16-bit microprocessor. Designed by a team led by Stephen Morse, the 8086 was Intel&amp;rsquo;s answer to the growing demand for more powerful processors that could handle larger programs and address more memory than the existing 8-bit chips of the era.&lt;/p>
&lt;p>The processor introduced the x86 architecture that would become the foundation for decades of computing evolution. With its 16-bit registers and 20-bit address bus &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, the 8086 could access up to 1 megabyte of memory—a massive improvement over the 64KB limitation of 8-bit processors. However, it retained backward compatibility concepts that would prove both beneficial and constraining for future generations.&lt;/p></description></item><item><title>Characteristics of MBR Code</title><link>https://sankethbk.github.io/blog/posts/boot/2025-07-07-characteristics-of-mbr-code/</link><pubDate>Sat, 12 Jul 2025 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/boot/2025-07-07-characteristics-of-mbr-code/</guid><description>&lt;h1 id="bios-boot-recap">BIOS Boot Recap&lt;/h1>
&lt;p>Previously, we saw that after the BIOS firmware is loaded, it searches for a bootable device from a list of storage options, such as a hard drive, SSD, USB, or network interface. The BIOS identifies a valid bootable device by checking for the &lt;code>0x55AA&lt;/code> signature at the end of the first sector. Once found, it loads the 512 bytes from this sector (LBA 0), which is known as the Master Boot Record (MBR).&lt;/p></description></item><item><title>What happens when you turn on computer?</title><link>https://sankethbk.github.io/blog/posts/boot/2025-07-02-how-computer-boots/</link><pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/boot/2025-07-02-how-computer-boots/</guid><description>&lt;h2 id="1-poweron--hardware-reset">1. Power‑On &amp;amp; Hardware Reset&lt;/h2>
&lt;h3 id="1-powergood-signal">1. Power‑Good Signal&lt;/h3>
&lt;p>The power supply stabilizes voltages and asserts a “Power‑Good” (PWR_OK) line to the motherboard. All devices receive power and begin to initialize themselves. The Central Processing Unit (CPU) is initially held in a reset mode, meaning it&amp;rsquo;s not yet executing instructions. The memory layout is powered up, although the RAM itself has no content since it&amp;rsquo;s volatile.&lt;/p>
&lt;h3 id="2-cpu-reset-vector">2. CPU Reset Vector&lt;/h3>
&lt;p>The reset vector is a predetermined memory address where the CPU begins execution after being powered on or reset. On x86 processors, this address is typically &lt;code>0xFFFFFFF0&lt;/code> (near the top of the 4GB address space). When the CPU comes out of reset, its program counter (instruction pointer) is automatically set to this address. The motherboard&amp;rsquo;s memory mapping ensures that this address points to the BIOS/UEFI firmware ROM chip, so the very first instruction the CPU executes comes from the firmware.&lt;/p></description></item><item><title>Representation of Negative Numbers in Hardware</title><link>https://sankethbk.github.io/blog/posts/binary/2024-07-21-representation-of-negative-numbers/</link><pubDate>Sun, 21 Jul 2024 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/binary/2024-07-21-representation-of-negative-numbers/</guid><description>&lt;p>Representing negative numbers in binary poses unique challenges due to the inherent nature of binary systems. Unlike decimal systems, which can easily use a minus sign to indicate negative values, binary systems must encode this information within a fixed number of bits. This requirement leads to various methods of representation, each with its own set of advantages and limitations. The main challenge lies in developing a system that can accurately represent both positive and negative values while ensuring that arithmetic operations remain efficient and straightforward. In the following sections, we will explore several common approaches to representing negative numbers in binary, including their respective challenges and trade-offs.&lt;/p></description></item><item><title>Overview of MIPS Assembly</title><link>https://sankethbk.github.io/blog/posts/cpu/2024-07-12-overview-of-mips-assembly/</link><pubDate>Fri, 12 Jul 2024 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/cpu/2024-07-12-overview-of-mips-assembly/</guid><description>&lt;p>MIPS (Microprocessor without Interlocked Pipeline Stages) assembly is one of the RISC ISA&amp;rsquo;s. It was developed in the early 1980s at Stanford University by Professor John L. Hennessy. MIPS is widely used in academic research and industry, particularly in computer architecture courses due to its straightforward design and in various embedded systems applications for its efficiency and performance.&lt;/p>
&lt;h2 id="history">History&lt;/h2>
&lt;p>The first MIPS processor, the R2000, was introduced. It implemented the MIPS I architecture, which was one of the earliest commercial RISC processors. There are multiple versions of MIPS: including MIPS I, II, III, IV, and V; as well as five releases of MIPS32/64. MIPS I had 32-bit architecture with basic instruction set and addressing modes. MIPS III introduced 64-bit architecture in 1991, increasing the address space and register width.&lt;/p></description></item><item><title>The Fetch Decode Execute Cycle</title><link>https://sankethbk.github.io/blog/posts/cpu/2024-06-30-the-fetch-decode-execute-cycle/</link><pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/cpu/2024-06-30-the-fetch-decode-execute-cycle/</guid><description>&lt;p>The Fetch-decode-execute cycle or instruction cycle is how CPU executes programs. During this cycle, the CPU retrieves an instruction from memory (fetch), interprets what action is required (decode), and then carries out the necessary operations to complete the instruction (execute). This cycle is crucial for the CPU to perform any computational tasks, and it repeats continuously while the computer is powered on.&lt;/p>
&lt;h2 id="what-is-machine-code">What is Machine Code?&lt;/h2>
&lt;p>Machine code is the lowest-level programming language that consists of binary instructions directly executed by a CPU. Any program is compiled to a binary executable is transformed into machine code. Machine code consists of set of instructions which varies for each CPU architecture and is decided by the CPU manufacturer, eg: ARM, MIPS, x86, etc. Machine code consists of a set of instructions defined by the Instruction Set Architecture (ISA) of each CPU. The ISA, determined by the CPU manufacturer, varies across different architectures such as ARM, MIPS, and x86. This architecture-specific design means that machine code written for one type of CPU cannot be directly executed on another without translation or emulation.&lt;/p></description></item><item><title>Measuring CPU Performance</title><link>https://sankethbk.github.io/blog/posts/cpu/2024-06-21-measuring-cpu-performance/</link><pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/cpu/2024-06-21-measuring-cpu-performance/</guid><description>&lt;p>CPU Manufacturers publish several metrics related to CPU like clock speed, number of cores, cache sizes, ISA, performance per Watt, number of transistors and more. Measuring CPU performance is complex, and it cannot be summarized by a single metric. In this post, I&amp;rsquo;ll explore each of these metrics and discuss some standard benchmarking software and their limitations.&lt;/p>
&lt;h2 id="what-is-clock-speed-how-does-it-affects-cpu-performance">What is Clock Speed, How does it Affects CPU Performance?&lt;/h2>
&lt;p>All Synchronous digital electronic circuits require an externally generated time reference. This is usually a square wave signal provided to the circuit called as clock. A &lt;strong>clock cycle&lt;/strong> is the fundamental unit of time measurement for a CPU. A clock cycle is a single electrical pulse in a CPU, during which the CPU can execute a fundamental operation such as accessing memory, writing data, or fetching a new set of instructions. A clock cycle is measured as the amount of time between two pulses of an oscillator. The clock speed of a CPU is measured in Hertz (Hz), which signifies the number of clock cycles it can complete in one second. Common units are Megahertz (MHz) and Gigahertz (GHz).&lt;/p></description></item><item><title>Key Differences between 32-bit and 64-bit CPU architectures</title><link>https://sankethbk.github.io/blog/posts/cpu/2024-06-02-processor-bit-size/</link><pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/cpu/2024-06-02-processor-bit-size/</guid><description>&lt;p>The terms 32 bit and 64 bit specifically relate to the size of the data and address registers within the CPU, which determines the maximum amount of memory that can be directly accessed and the range of values that can be processed.&lt;/p>
&lt;h3 id="1-registers-and-data-width">1. Registers and Data Width:&lt;/h3>
&lt;ul>
&lt;li>Since all calculations take place in registers, when performing operations such as addition or subtraction, variables are loaded from memory into registers if they are not already there.&lt;/li>
&lt;li>A 32-bit CPU has 32-bit wide registers, meaning it can process 32 bits of data in a single instruction.&lt;/li>
&lt;/ul>
&lt;h3 id="2-memory-addressing">2. Memory Addressing:&lt;/h3>
&lt;ul>
&lt;li>32-bit CPU can address up to 2&lt;sup>32&lt;/sup> unique memory locations translates to a maximum of 4 GB of addressable memory (RAM). 64-bit CPU can address up to 2&lt;sup>64&lt;/sup> unique memory locations allowing for a theoretical maximum of 16 exabytes of addressable memory.&lt;/li>
&lt;li>This limitation comes from the fact that a 32-CPU can only load integers that are 32 bits long, thus limiting the maximum addressable memory space.&lt;/li>
&lt;/ul>
&lt;h3 id="3-data-transfer-speeds">3. Data Transfer Speeds:&lt;/h3>
&lt;ul>
&lt;li>The memory bus width in 64-bit CPU is often 64 bits or more, meaning the physical path between the CPU and RAM can handle 64 bits of data in parallel. This helps in efficiently loading data into the cache but does not restrict the CPU to always reading 64 bits.&lt;/li>
&lt;li>Despite the ability to handle 64 bits of data in parallel, the CPU is not restricted to always reading 64 bits at a time. It can access smaller data sizes (e.g., 8-bit, 16-bit, 32-bit) as needed, depending on the specific instruction and data type.&lt;/li>
&lt;/ul>
&lt;h3 id="4-performance">4. Performance:&lt;/h3>
&lt;ul>
&lt;li>64-bit CPU&amp;rsquo;s perform better than 32-bit CPU&amp;rsquo;s. This performance difference comes up from various factors like size of registers, addressable memory space, larger bus width&lt;/li>
&lt;li>Some RISC architectures support SIMD (Single Instruction, Multiple Data) instructions that allow for parallel processing of multiple smaller data types within larger registers. For example, ARM&amp;rsquo;s NEON technology can operate on multiple 32-bit integers within 64-bit registers, which enable the parallel processing of smaller data types within larger registers.&lt;/li>
&lt;/ul>
&lt;h3 id="5-application-compatibility">5. Application Compatibility:&lt;/h3>
&lt;ul>
&lt;li>64-bit operating systems typically include backward compatibility to run 32-bit software seamlessly.&lt;/li>
&lt;li>These compatibility layers allow 32-bit applications to execute on 64-bit systems without any major issues. However, 32-bit applications may not fully utilize the advantages of 64-bit systems, such as increased memory addressing capabilities.&lt;/li>
&lt;/ul></description></item><item><title>Components of CPU</title><link>https://sankethbk.github.io/blog/posts/cpu/2024-05-31-components-of-cpu/</link><pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate><guid>https://sankethbk.github.io/blog/posts/cpu/2024-05-31-components-of-cpu/</guid><description>&lt;p>Before learning Assembly, I think it would be useful to learn a bit about different components of CPU in general. If we think of CPU as a black box its main function is to fetch instructions from RAM which are in the form of &lt;a href="https://en.wikipedia.org/wiki/Machine_code">machine code&lt;/a> and execute them.&lt;/p>
&lt;h2 id="components-of-cpu">Components of CPU&lt;/h2>
&lt;ol>
&lt;li>Arithmetic Logic Unit (ALU)&lt;/li>
&lt;li>Memory Management Unit (MMU)&lt;/li>
&lt;li>Control Unit (CU)&lt;/li>
&lt;li>Registers&lt;/li>
&lt;li>Clock&lt;/li>
&lt;li>Cache&lt;/li>
&lt;li>Buses&lt;/li>
&lt;/ol>
&lt;h3 id="1-arithmetic-logic-unit-alu">1. Arithmetic Logic Unit (ALU)&lt;/h3>
&lt;p>ALU is an electronic circuit made of NAND gates responsible for performing arithmetic and logical operations on integer binary numbers. It takes two operands as inputs and an opcode to indicate the type of operation to be performed. Operations supported by ALU are Add, Subtract, Negation, Two&amp;rsquo;s complement, AND, OR, XOR, bit shift, etc.&lt;/p></description></item></channel></rss>